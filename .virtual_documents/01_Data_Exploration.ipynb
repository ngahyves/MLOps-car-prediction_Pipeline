





import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
data=pd.read_csv('vehicles.csv')
df=data


df.head()





# Shape of our data set
shape= df.shape
print(f"Our data set has {shape[0]} rows and {shape[1]} columns")


# Summary statistics
df.describe(include="all")


# General information about our data set
df.info()








# Checking for duplicates
duplicates=df[df.duplicated()]
print(duplicates)








# Checking for missing values

missing=df.isnull().sum()
missing=missing.sort_values(ascending=False)

missing_prop=df.isnull().sum()/len(df)*100
missing_prop=missing_prop.sort_values(ascending=False)

missing_table=pd.DataFrame({
    'Count':missing,
    'Percentage': missing_prop
})

print(missing_table)





# ---Droping unnecessary columns---
cols_to_drop = ['county', 'size', 'VIN', 'description', 'image_url', 'url', 'posting_date', 'lat', 'long','region_url']
df.drop(columns=cols_to_drop, inplace=True)

#---Removing rows with missing value in most important columns---
essential_cols = ['year', 'manufacturer', 'model','type', 'odometer', 'fuel', 'transmission', 'title_status']
df.dropna(subset=essential_cols, inplace=True)

# ---Replacing NaN by "unknown" in other categorical variables---
cols_to_fill = ['cylinders', 'condition', 'drive', 'paint_color', 'type']
for col in cols_to_fill:
    df[col]=df.fillna('unknown', inplace=True)

#--- Verification---

print("\nTable after removing missings :")
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100
missing_info = pd.DataFrame({'Count': missing_values, 'Percentage': missing_percentage})
print(missing_info)






#--- Changing the 'year' data type in year instead of float---

df.year=pd.to_datetime(df['year'], errors='coerce').dt.date
df.info() # to verify the data type change





df.drop(columns=['cylinders','paint_color','condition','drive', 'type'], inplace=True) 



# Reseting the index
df = df.reset_index(drop=True)
df








df = df[df['price'] >= 500].copy()


df.info()





print(df.model.value_counts().describe())


#---Cardinality reduction---
model_counts = df['model'].value_counts()

# --- Finding the best thereshold ---
# We want to find the number of models that represent 85% of all advertisements.
target_percentage = 0.85
total_rows = len(df)

cumulative_sum = model_counts.cumsum()
num_models_to_keep = (cumulative_sum < total_rows * target_percentage).sum() + 1

print(f"To cover {target_percentage*100}% of our data, we have to keep the {num_models_to_keep} most frequent models.")

# Now, let's find the corresponding frequency threshold.

if num_models_to_keep < len(model_counts):
    
    ideal_threshold = model_counts.iloc[num_models_to_keep - 1] 
    print(f"The ideal frequency threshold to achieve this goal is : {ideal_threshold}")
else:
    print("All models are importants.")

# --- Apply the new thereshold ---

if 'ideal_threshold' in locals():
    # creating a copy
    df_processed = df.copy()
    
    rare_models = model_counts[model_counts < ideal_threshold].index
    df_processed['model'] = df_processed['model'].replace(rare_models, 'other')
    
    final_model_count = df_processed['model'].nunique()
    print(f"\nWith thershold of{ideal_threshold}, The number of unique models is now : {final_model_count}")
    





# 1. Number of models to keep
N = 200 

# 2. N most frequent models
top_n_models = model_counts.head(N).index
print(f"we will keep {N} most frequent models.")

import numpy as np

# If a model is in our top_n_models list, we keep it; otherwise, we replace it with “other”..
df['model_reduced'] = np.where(df['model'].isin(top_n_models), df['model'], 'other')

# --- Verification ---
final_model_count = df['model_reduced'].nunique()
print(f"\nThe final number of categories for the model is now : {final_model_count}") 


print("\nDistribution of new categories :")
print(df['model_reduced'].value_counts())

# "Other percentage
other_percentage = (df['model_reduced'] == 'other').mean() * 100
print(f"\nPercentage of ads in the category 'other' : {other_percentage:.2f}%")








# ---Boxplot to see the outliers of numerical columns---

df_num=df.select_dtypes(include='number')
fig, axes = plt.subplots(1, len(df_num.columns), figsize=(6 * len(df_num.columns), 7))

# If we have just one numerical column, axes will not be a list
if len(df_num.columns) == 1:
    axes = [axes]

for i, col in enumerate(df_num.columns):
    sns.boxplot(y=df_num[col], ax=axes[i])
    axes[i].set_title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()






# Detecting outliers using Inter quartile range and capped them
print(f"Size of the DataFrame before outlier processing : {len(df)}")

# --- Step 1: FILTERING outliers from the target variable 'price' ---

# We remove the lowest 1% and the highest 1% of prices.
price_lower_bound = df['price'].quantile(0.01)
price_upper_bound = df['price'].quantile(0.99)

print(f"\nFiltering prices outside the range [{price_lower_bound:.2f}, {price_upper_bound:.2f}]...")

# Only lines within these boundaries are retained.
df_filtered = df[(df['price'] >= price_lower_bound) & (df['price'] <= price_upper_bound)].copy()

print(f"DataFrame size after filtering on price : {len(df_filtered)}")
print(f"{len(df) - len(df_filtered)} rows deleted.")

# --- Step 2: CAPPING outliers on OTHER numerical variables ---

def cap_outliers_except_target(df_in, target_col='price'):
    """
    Applies IQR capping to all numeric columns EXCEPT the target column.
    """
    df = df_in.copy()
    
   # Select the numerical columns to be processed (all except the target)
    numeric_cols_to_cap = df.select_dtypes(include=np.number).columns.drop(target_col, errors='ignore')
    
    print(f"\n Capping Application  : {list(numeric_cols_to_cap)}")
    
    for col in numeric_cols_to_cap:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        df[col] = np.clip(df[col], a_min=lower_bound, a_max=upper_bound)
        
    return df

# Apply the capping function to our already filtered DataFrame
df_final = cap_outliers_except_target(df_filtered)

print("\nOutlier processing completed.")


df_final.select_dtypes(include='number').head()











#Summary statistics
df_final_num = df_final.select_dtypes(include='number').copy()
df_final_num['yr'] = pd.to_datetime(df_final['year'], errors='coerce').dt.year

df_final_num.describe(include='all')


fig, axes = plt.subplots(1, len(df_final_num.columns), figsize=(6 * len(df_final_num.columns), 7))

# If we have just one numerical column, axes will not be a list
if len(df_final_num.columns) == 1:
    axes = [axes]

for i, col in enumerate(df_final_num.columns):
    sns.histplot(x=df_final_num[col], kde=True, ax=axes[i])
    axes[i].set_title(f'histogram of {col}')
    axes[i].set_xlabel(col)

plt.tight_layout()
plt.show()





#---Q-Q plot for the price
from scipy import stats

plt.figure()
stats.probplot(df_final['price'], plot=plt)
plt.title('Q-Q plot price')
plt.show()


#We can also verify it with shapito wilk test and kolgomorov smirvov
from scipy.stats import shapiro
from scipy.stats import kstest, norm

# Shapiro test
shapiro_test = shapiro(df_final['price'])
p_value = shapiro_test.pvalue
print('shapiro p value= ', p_value)

#ks test
standardized = (df_final['price'] - df_final['price'].mean()) / df_final['price'].std()

# KS test
ks_test = kstest(standardized, 'norm')
print('ks test p value= ',ks_test.pvalue)








df_final_cat=df_final.select_dtypes(include='object')



#Countplot for categorical variables

import matplotlib.pyplot as plt
import seaborn as sns
#List of categorical variables
df_categorical_cols = df_final.select_dtypes(include='object').columns.tolist()

def plot_categorical_distributions(df, categorical_cols, top_n=15):
    """
    Displays the distribution of categorical variables.
For variables with many categories, only displays the top N
    """
    n_cols = 2
    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols
    
    plt.figure(figsize=(n_cols * 8, n_rows * 5))

    for i, col in enumerate(categorical_cols):
        ax = plt.subplot(n_rows, n_cols, i + 1)
        
        # Decide on the visualisation strategy based on the number of categories
        if df[col].nunique() > top_n:
            # --- High Cardinality: Display Top N ---
            # We take the N most frequent categories
            top_categories = df[col].value_counts().nlargest(top_n).index
            
            # We filter the DataFrame to keep only these categories.
            df_top = df[df[col].isin(top_categories)]
            
            # plotting the graphs
            sns.countplot(data=df_top, y=col, order=top_categories, palette='viridis', ax=ax)
            ax.set_title(f'Top {top_n} for the variable "{col}" (High Cardinality)')
            ax.set_xlabel('adds')
            ax.set_ylabel(col.capitalize())
            
        else:
            # --- Low Cardinality: Show all ---
            order = df[col].value_counts().index
            sns.countplot(data=df, x=col, order=order, palette='Set2', ax=ax)
            ax.set_title(f'Distribution of the variable "{col}"')
            ax.set_xlabel(col.capitalize())
            ax.set_ylabel('Adds')
            plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

# --- calling the function ---
plot_categorical_distributions(df_final, df_categorical_cols)








# Pairplot between price and odometer
sns.pairplot(df_final_num[['odometer', 'price', 'yr']])
plt.show()


# Spearman correlation test
from scipy.stats import spearmanr

# Spearman test
rho, p_value = spearmanr(df_final['price'], df_final['odometer'])

print(f"Spearman coeff : {rho}")
print(f"P-value : {p_value}")









# Create the logarithmic transformation on the final DataFrame
df_final['price_log'] = np.log1p(df_final['price'])

# View the distribution and the Q-Q plot
print("\nValidation of the distribution of “price_log” after final processing :")

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Histogram
sns.histplot(df_final['price_log'], kde=True, bins=50, ax=axes[0])
axes[0].set_title('Distribution of ‘price_log’ (Final)')

# Q-Q Plot
stats.probplot(df_final['price_log'], dist="norm", plot=axes[1])
axes[1].set_title('Q-Q Plot of "price_log" (Finale)')

plt.tight_layout()
plt.show()

#Display information about the final DataFrame that will be used for modelling
print("\nInformation about the final DataFrame ready for modelling :")
df_final.info()


df_final.to_csv('df_cleaned.csv', index=False)



